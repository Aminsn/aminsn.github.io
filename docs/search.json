[
  {
    "objectID": "blogs/2025/bayesian-singleton-take/index.html",
    "href": "blogs/2025/bayesian-singleton-take/index.html",
    "title": "Stop Deleting Your Data: Let Bayesian Models Handle It",
    "section": "",
    "text": "Let’s say you’re trying to measure the impact of dozens of products, features, or treatments. You run your GROUP BY query and find the usual mess: some products are used by thousands of customers, and others… well, others have been used exactly once by one specific customer.\nThis is the classic singleton problem, and it’s a pain. The effect of that one-off product is perfectly confounded with the unique quirks of that one-off customer. Is the product amazing, or is the customer just a hyper-performer? You can’t tell.\nThe standard playbook? Just filter them out.\nIt feels clean, but it’s a trap. Dropping that data means:\nInstead of making an arbitrary call to delete data, we can build a model that’s smart enough to handle the mess. This is a perfect job for a Bayesian hierarchical model."
  },
  {
    "objectID": "blogs/2025/bayesian-singleton-take/index.html#modeling-our-skepticism",
    "href": "blogs/2025/bayesian-singleton-take/index.html#modeling-our-skepticism",
    "title": "Stop Deleting Your Data: Let Bayesian Models Handle It",
    "section": "Modeling Our Skepticism",
    "text": "Modeling Our Skepticism\nLet’s start with a standard hierarchical (or mixed-effects) model. We’ll model our target variable \\(y\\) with a structure that includes fixed effects and random effects for customers and products, and we make the likelihood explicit:\n\\[\ny_i \\sim \\mathrm{Normal}(\\mu_i, \\sigma_\\varepsilon^2), \\qquad\n\\mu_i = \\alpha + \\boldsymbol{Z}_i^{\\mathsf T} \\gamma + \\lambda_{c[i]} + \\beta_{p[i]} \\, .\n\\]\n\n\\(\\alpha\\) is our global intercept.\n\\(\\boldsymbol{Z}_i^{\\mathsf T} \\gamma\\) handles fixed effects (e.g., a customer’s age).\n\\(\\lambda_{c}\\) is a random effect that soaks up each customer’s baseline tendencies.\n\\(\\beta_{p}\\) is the random effect for the product, the lift we actually care about.\n\nThe model assumes each \\(\\beta_{p}\\) is drawn from a common distribution, typically \\(\\mathrm{Normal}(0,\\sigma_\\beta^2)\\). This means each product’s effect is “shrunk” toward the average effect across products, and only strong evidence in the data will pull its estimate away from the mean.\nBut for a true singleton (one user, one product, no other overlap), the model still can’t disentangle \\(\\lambda_{c}\\) from \\(\\beta_{p}\\) from the likelihood alone. Only the sum \\(\\lambda_{c^\\ast}+\\beta_{p^\\ast}\\) is identified; the individual effects are not separately identifiable.\n\nIsolated dyads (no within-customer contrast)\nIf a product \\(p^\\ast\\) is used only by a single customer \\(c^\\ast\\) and that customer does not use any other product (there may be many observations of this pair), a simpler model without an interaction can already behave skeptically. With a tighter prior on \\(\\sigma_\\beta\\) compared to \\(\\sigma_\\lambda\\), the posterior will typically attribute the pair’s signal to the customer baseline \\(\\lambda_{c^\\ast}\\) and keep \\(\\beta_{p^\\ast}\\) near zero. In this isolated-dyad setting, an interaction term is optional."
  },
  {
    "objectID": "blogs/2025/bayesian-singleton-take/index.html#the-real-trick-priors-on-an-interaction-term-when-its-needed",
    "href": "blogs/2025/bayesian-singleton-take/index.html#the-real-trick-priors-on-an-interaction-term-when-its-needed",
    "title": "Stop Deleting Your Data: Let Bayesian Models Handle It",
    "section": "The Real Trick: Priors on an Interaction Term (when it’s needed)",
    "text": "The Real Trick: Priors on an Interaction Term (when it’s needed)\nWhere the previous model can go wrong is when we have a singleton product used by a customer who also uses other products. In that case, we observe a within-customer contrast:\n\\[\ny_{c^*,p^*} - y_{c^*,q} \\approx \\beta_{p^*} - \\beta_q \\, ,\n\\]\nso any one-off bump unique to \\(p^*\\) for that customer cannot be absorbed by \\(\\lambda_{c^*}\\), which cancels out. Without further structure, the model may attribute that unique blip to a general product effect \\(\\beta_{p^*}\\) even if it lacks corroboration across other customers. While it’s entirely possible that \\(\\beta_{p^*}\\)’s effect is genuine, it could just as well be driven by unaccounted confounders. For example, the customer changing their lifestyle at the same time they adopted the new product.\nThe question becomes: do we trust that the product is truly different from the average solely based on this single customer’s outcome, or do we remain skeptical unless the effect is confirmed by multiple customers? I prefer the latter stance. To enable this, we give the model another place to put the blame by adding a customer–product interaction term, \\(\\eta_{p,c}\\):\n\\[\n\\mu_i = \\alpha + \\boldsymbol{Z}_i^{\\mathsf T} \\gamma + \\lambda_{c[i]} + \\beta_{p[i]} + \\eta_{p[i],c[i]} \\, .\n\\]\nNow, for any given observation, the model can attribute a large effect either to the product in general (\\(\\beta_p\\)) or to the unique combination of that specific customer using that specific product (\\(\\eta_{p,c}\\)).\nThis is where the priors do the heavy lifting. We are opinionated about the variance components:\n\nFor the interaction effects, \\(\\eta_{p,c}\\sim \\mathrm{Normal}(0,\\sigma_\\eta^2)\\), we use a loose prior on \\(\\sigma_\\eta\\). This tells the model: “One-off, idiosyncratic outcomes can occur when a specific person uses a specific product. Use this term to explain large effects that aren’t corroborated by other users.”\nFor the main product effects, \\(\\beta_{p}\\sim \\mathrm{Normal}(0,\\sigma_\\beta^2)\\), we place a tighter prior on \\(\\sigma_\\beta\\) compared to \\(\\sigma_\\eta\\). We’re telling the model: “I’m skeptical. Don’t move a \\(\\beta_{p}\\) far from zero unless there’s strong, consistent evidence across multiple customers.”\nCustomer baselines \\(\\lambda_c\\sim \\mathrm{Normal}(0,\\sigma_\\lambda^2)\\) are regularized with a sensible (not overly loose) prior. This helps in sparse settings and ensures \\(\\lambda_c\\) captures persistent, across-product tendencies."
  },
  {
    "objectID": "blogs/2025/bayesian-singleton-take/index.html#the-payoff-a-model-that-tells-the-truth",
    "href": "blogs/2025/bayesian-singleton-take/index.html#the-payoff-a-model-that-tells-the-truth",
    "title": "Stop Deleting Your Data: Let Bayesian Models Handle It",
    "section": "The Payoff: A Model That Tells the Truth",
    "text": "The Payoff: A Model That Tells the Truth\nThis setup forces the model to behave exactly as we’d want a skeptical analyst to.\nIf multiple customers use product \\(A\\) and consistently see a positive lift, the data provides a clear, shared signal. The model will confidently estimate a positive \\(\\beta_A\\) with a tight posterior distribution.\nBut what about that singleton product, \\(B\\), used by a customer (e.g. \\(\\text{user123}\\)) who also uses others? The single heavy user might have a massive outcome for \\(B\\). The model, however, sees no corroborating evidence. Because the prior on \\(\\sigma_\\beta\\) makes it “expensive” to declare a large general product effect, the model will find it much “cheaper” to explain the outlier result using the flexible interaction term, \\(\\eta_{B,\\text{user123}}\\).\nThe result? The posterior for \\(\\beta_B\\) will remain shrunk close to zero with appropriately wide uncertainty. The model is effectively telling you, “Based on the data I have, I can’t distinguish this product’s effect from a random user-specific fluke.”\nAnd that is a much more useful and honest answer than a noisy point estimate from a single observation. It allows you to keep all your data while building a model that learns what can be learned and transparently reports its uncertainty about what can’t."
  },
  {
    "objectID": "blogs/2023/inla-random-walk/index.html",
    "href": "blogs/2023/inla-random-walk/index.html",
    "title": "Modeling Non-linear Effects using Random Walk in R-INLA",
    "section": "",
    "text": "Random walk models can be very useful for modeling non-linear effects in statistical analysis. In this post, we will explore how to use the Integrated Nested Laplace Approximation (INLA) method with a random walk model in R to show how to model non-linear relationships.\n\nRequirements\nWe’ll need to load the INLA library in R. If you don’t have R-INLA installed, you can follow the instructions here to install it.\n\n\nRandom Walk: A Brief Overview\nImagine a stock price that moves daily. You might think today’s price is the best predictor of tomorrow’s price, with some random fluctuation added. This idea is central to a random walk model. Mathematically, this can be expressed as:\n\\[P_{t}  = P_{t-1} + \\epsilon_{t}\\] where:\n\n\\(P_{t}\\) is the price at time \\(t\\).\n\\(\\epsilon_{t}\\) is a random error term at time \\(t\\), typically assumed to be normally distributed with mean 0.\n\nThis model suggests that the change in value from one time point to the next is purely random, hence the name random walk.\n\n\nUsing Random Walk to Model Non-Linearity\nWhile random walks are mostly used in time series analysis (like in the stock price example) but the idea could be easily applied to problems where there is no time dimension. In the following example we will see how we can use random walk to recover a non-linear relationship between two continuous variables.\nSuppose x is continuous and uniformly distributed between [-100, 100], represented as:\n\\[x \\sim \\text{Uniform}(-100, 100)\\]\nWe then describe y as follows:\n\\[y = f(x) + \\epsilon\\] where \\(f(x)\\) represents a non-linear function, and \\(\\epsilon\\) is the error term. For this example let’s assume:\n\\[f(x) = \\frac{1}{1+e^{-0.1x}}\\] \\[\\epsilon \\sim N(0, 0.3^2)\\]\nWe will use R to simulate data according to the above specifications:\n\nlibrary(ggplot2)\n\n# define the sigmoid function\nsigmoid = function(x){1/(1+exp(-x))}\n\n# simulate data\nN = 100 # num of observations\nset.seed(1)\nx = sort(runif(N, -100 , 100))\neps = rnorm(N,0,0.05) \nfx = sigmoid(.1*x)\ny = fx + eps\n\ndf = data.frame(x=x, y=y, fx = fx)\n\n# plotting the data\nggplot(df, aes(x,y)) + \n  geom_point() + \n  geom_line(aes(x,fx), color = 'red') + \n  theme_classic() + \n  theme(\n    panel.background = element_rect(fill = \"#FDFBF7\"),\n    plot.background = element_rect(fill = \"#FDFBF7\"),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank(),\n    text = element_text(family = \"Comic Sans MS\")\n  ) + \n  labs(x = 'X', y = 'Y', title = 'Scatter plot of x vs y and the true underlying function')\n\n\n\n\n\n\n\n\nThe red line in the plot above shows the target function we’re aiming to infer from the data.\n\n\nInference\nTo infer the true relationship, we can model the data generating process using a random walk. To do so, first we need to discretize x into \\(k\\) bins to get \\({x_{1},...,{x_{k}}}\\) discrete points (more on binning here). We then define our model as follows:\n\\[y_{i} \\sim N(\\beta_{x_{i}}, \\sigma^2)\\] \\[\\beta_{x_{i}} \\sim N(\\beta_{x_{i-1}}, \\sigma_{f}^2)\\]\nwhere \\(E(y_{i}) = \\beta_{x_{i}}\\) and \\(E(\\beta_{x_{i}}) = \\beta_{x_{i-1}}\\). After defining the model, we must decide on the inference method, such as MLE, MCMC, etc. In this case, we employ the Laplace approximation using INLA. It’s important to note that INLA adopts a Bayesian approach, which requires us to define priors for our model parameters. For this example, we’ll stick with INLA’s default settings. To learn more about them, simply use the inla.doc(\"model_name\") command, for example, inla.doc(\"rw1\") for the random walk model.\n\nlibrary(INLA)\n\n# binning the predictor\ndf$x_binned = inla.group(df$x, n = 10, method = \"cut\")\n\n# model definition\nformula = y ~ -1 + f(x_binned, model = 'rw1', constr=FALSE)\n\n# fitting the model\nmodel = inla(formula, data = df)\n\n# extracting the estimated effects\nbin_center = model$summary.random$x_binned$ID\nmean = model$summary.random$x_binned$mean\nlower = model$summary.random$x_binned$`0.025quant`\nupper = model$summary.random$x_binned$`0.975quant`\n\ndf_pred = data.frame(bin_center, mean, lower, upper)\n\nggplot() + \n  geom_ribbon(data = df_pred, \n              aes(x = bin_center, ymin = lower, ymax = upper), \n              fill = \"grey70\", alpha = 0.6) + \n  geom_point(data = df, aes(x,y)) + \n  theme_classic() + \n  geom_line(data = df_pred, aes(bin_center, mean), color ='red') + theme(\n    panel.background = element_rect(fill = \"#FDFBF7\"),\n    plot.background = element_rect(fill = \"#FDFBF7\"),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank(),\n    text = element_text(family = \"Comic Sans MS\")\n  ) + \n  labs(x = 'X', y = 'Y', title = 'Scatter plot of x vs y and the fitted function')\n\n\n\n\n\n\n\n\nAs you can see in the plot above, the estimated function, represented in red, provides a good approximation of the true function. This suggests that the random walk model effectively captures the original function.\n\n\nQuick Recap\n\nRandom Walk Model: A simple yet effective model that assumes the change in value from one time point to the next is purely random.\nApplication Beyond Time Series: Though commonly associated with time series data, the random walk model can be applied to scenarios without a time dimension.\nPractical Implementation: We provided a simple example where we simulated data based on a given relationship and then used the random walk approach with INLA to recover the original relationship from the data."
  },
  {
    "objectID": "blogs/index.html",
    "href": "blogs/index.html",
    "title": "Amin S.Nejad",
    "section": "",
    "text": "Stop Deleting Your Data: Let Bayesian Models Handle It\n\n\n\n\n\n\n\nBayesian Statistics\n\n\n\n\nI once saw priors as a hurdle, now I see them as the key to honest, data-rich modeling.\n\n\n\n\n\n\nSeptember 7, 2025\n\n\nAmin Shoari Nejad\n\n\n\n\n\n\n  \n\n\n\n\nModeling Non-linear Effects using Random Walk in R-INLA\n\n\n\n\n\n\n\nR\n\n\nBayesian Statistics\n\n\nINLA\n\n\nRandom Walk\n\n\n\n\nA guide to modeling non-linear effects using random walk with the R-INLA package.\n\n\n\n\n\n\nAugust 6, 2023\n\n\nAmin Shoari Nejad\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Amin S.Nejad",
    "section": "",
    "text": "Twitter\n  \n  \n    \n     GitHub\n  \n  \n    \n     LinkedIn\n  \n  \n    \n     Email\n  \n\n  \n  \n\n\nI’m Amin, a dedicated Data Scientist with a knack for unraveling the ‘why’ with statistical models and visualisation techniques and predict the ‘what’ using machine learning and deep learning models. My passion for data transcends the entire pipeline. I find thrill in every step, from the nitty-gritty of data cleaning (surprising, isn’t it?) to the exhilarating task of transforming raw data into actionable insights. With each dataset, I aim to uncover hidden patterns, unlock value, and craft a compelling data-driven story. Welcome to my world, where data isn’t just numbers, but a catalyst for change.\n\n\n Coding – Data Science – Machine Learning – Bayesian Statistics"
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Amin S.Nejad",
    "section": "",
    "text": "I’m Amin, a dedicated Data Scientist with a knack for unraveling the ‘why’ with statistical models and visualisation techniques and predict the ‘what’ using machine learning and deep learning models. My passion for data transcends the entire pipeline. I find thrill in every step, from the nitty-gritty of data cleaning (surprising, isn’t it?) to the exhilarating task of transforming raw data into actionable insights. With each dataset, I aim to uncover hidden patterns, unlock value, and craft a compelling data-driven story. Welcome to my world, where data isn’t just numbers, but a catalyst for change.\n\n\n Coding – Data Science – Machine Learning – Bayesian Statistics"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  }
]